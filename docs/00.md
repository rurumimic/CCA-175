1. [Spark](#spark)
1. [RDD, DataFrame, Dataset](#rdd-dataframe-dataset)
   1. [RDD](#rdd)
   1. [DataFrame](#dataframe)
   1. [Dataset](#dataset)
1. [Import](#import)
1. [Export](#export)

## Spark

```scala
spark // org.apache.spark.sql.SparkSession
sc // org.apache.spark.SparkContext
```

## RDD, DataFrame, Dataset

### RDD

```scala
val data = Seq(1,2,3,4,5)
val rdd = sc.parallelize(data)

rdd.partitions.size // 2
rdd.collect.foreach(x => print(x + ",")) // 1,2,3,4,5,

rdd.toDF
rdd.toDS
```



### DataFrame

rdd to df programmatically:

```scala
// Raw RDD
val data = Seq("Jacek 42", "Patryk 19", "Maksym 5")
val rdd = sc.parallelize(data)

// Prepare a Schema
import org.apache.spark.sql.types._
val schemaString = "name age"
val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true))
val schema = StructType(fields)

// Prepare a rdd
import org.apache.spark.sql.Row
val rowRDD = rdd.map(_.split(" ")).map(x => Row(x(0), x(1)))
// rowRDD.collect // = Array([Jacek,42], [Patryk,19], [Maksym,5])

// Convert to DataFrame
val df = spark.createDataFrame(rowRDD, schema)

df.show

+------+---+                                                                    
|  name|age|
+------+---+
| Jacek| 42|
|Patryk| 19|
|Maksym|  5|
+------+---+

df.printSchema

root
 |-- name: string (nullable = true)
 |-- age: string (nullable = true)
```

```scala
case class Person(name: String, age: Int)
val people = Seq(Person("Jacek", 42), Person("Patryk", 19), Person("Maksym", 5))
val df = spark.createDataFrame(people)

Seq(1, 2, 3).toDF() // org.apache.spark.sql.DataFrame = [value: int]
```

### Dataset

RDD to Dataset:

```scala
// import spark.implicits._
val data = Seq(("Java", 20000), ("Python", 100000), ("Scala", 3000))
val rdd = spark.sparkContext.parallelize(data)
val ds = rdd.toDS()
ds.show

case class Languages(name: String, number: Int)
val ds = rdd.toDF("name", "number").as[Languages]
ds.show
```

```scala
// case class
case class Person(name: String, age: Long)
val caseClassDS = Seq(Person("Andy", 32), Person("Thomas", 17)).toDS()

// implicits
val primitiveDS = Seq(1, 2, 3).toDS() // org.apache.spark.sql.Dataset[Int] = [value: int]

// dataframe to dataset
caseClassDS.write.parquet("/tmp/people")
spark.read.parquet("/tmp/people").show // org.apache.spark.sql.DataFrame = [name: string, age: bigint]
spark.read.parquet("/tmp/people").as[Person]
```

## Import

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read
.option("inferSchema", true)
.option("delimiter", "\t")
.csv("/user/vagrant/db/customers-tab-delimited")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val orders = spark.read.parquet("/user/vagrant/db/orders_parquet")
val products = spark.read.format("avro").load("/user/vagrant/db/products_avro")
```

```scala
// spark.sparkContext.textFile...
val lines = sc.textFile("/user/vagrant/db/orders")
val lines = sc.textFile("/user/vagrant/db/customers-tab-delimited")
// lines: org.apache.spark.rdd.RDD[String]

```

## Export

```scala
```
