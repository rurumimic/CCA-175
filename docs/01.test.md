# Test 1

```bash
spark-shell --master yarn --deploy-mode client --driver-memory 2g
```

## Problem 1

### Data

- data: `/user/vagrant/db/customers-tab-delimited`
- format: text
- delimited: tab

### Schema

```scala
customer_id             int   
customer_fname          string
customer_lname          string
customer_email          string
customer_password       string
customer_street         string
customer_city           string
customer_state          string
customer_zipcode        string
```

### Output

```scala
// import spark.implicits._
```

use `:paste`

```scala
val data = spark.read.option("inferSchema", true)
  .option("delimiter", "\t")
  .csv("/user/vagrant/db/customers-tab-delimited")
  .toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
/*
// Rename:
val data = spark.read. ... .toDF()

val columns = Seq("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")

val data2 = data.toDF(columns: _*)
*/
```

```scala
val result = data.filter($"customer_state" === "CA")
.select(concat_ws(" ", $"customer_fname", $"customer_lname").alias("customer_name"))
```

```scala
data.createOrReplaceTempView("customer_view")
val result = spark.sql("""
select concat_ws(' ', customer_fname, customer_lname) customer_name
from customer_view
where customer_state = 'CA'
""")
```

```scala
result.write.text("/user/vagrant/test1/problem1/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test1/problem1/solution
hdfs dfs -cat /user/vagrant/test1/problem1/solution/* | head
```

## Problem 2

### Data

- data: `/user/vagrant/db/orders_parquet`
- format: parquet

### Output

```scala
val data = spark.read.parquet("/user/vagrant/db/orders_parquet")
```

```scala
val result = data
.filter($"order_status" === "COMPLETE")
.select($"order_id", $"order_date", $"order_status")
.withColumn("order_date", to_date(from_unixtime($"order_date" / 1000)))
```

```scala
// spark.sql("show tables").show()
data.createOrReplaceTempView("order_view")
val result = spark.sql("""
select order_id, to_date(from_unixtime(order_date / 1000)) order_date, order_status
from order_view
where order_status = 'COMPLETE'
""")
```

```scala
result.write
.option("compression", "gzip")
.json("/user/vagrant/test1/problem2/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test1/problem2/solution
```

## Problem 3

### Data

- data: `/user/vagrant/db/customers-tab-delimited`
- format: text
- delimited: tab

### Schema

```scala
customer_id             int   
customer_fname          string
customer_lname          string
customer_email          string
customer_password       string
customer_street         string
customer_city           string
customer_state          string
customer_zipcode        string
```

### Output

```scala
val data = spark.read
  .option("inferSchema", true)
  .option("delimiter", "\t")
  .csv("/user/vagrant/db/customers-tab-delimited")
  .toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val result = data.filter($"customer_city" === "Caguas")
```

```scala
result.write
.option("compression", "snappy")
.orc("/user/vagrant/test1/problem3/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test1/problem3/solution
```

## Problem 4

### Data

- data: `/user/vagrant/db/categories`
- format: text
- delimited: comma

### Schema

```scala
category_id             int                                         
category_department_id  int                                         
category_name           string 
```

### Output

```scala
val data = spark.read
.option("inferSchema", "true")
.csv("/user/vagrant/db/categories")
.toDF("category_id", "category_department_id", "category_name")
```

```scala
data
.map(row => row.mkString("\t"))
.write
.option("compression", "lz4")
.text("/user/vagrant/test1/problem4/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test1/problem4/solution
hdfs dfs -cat /user/vagrant/test1/problem4/solution/* | head
```

## Problem 5

spark-shell option: `--packages org.apache.spark:spark-avro_2.12:2.4.7`

### Data

- data: `/user/vagrant/db/products_avro`
- format: avro
- compression: snappy

### Output

```scala
val data = spark.read.format("avro").load("/user/vagrant/db/products_avro")
```

```scala
data
.filter($"product_price" > 1000.0)
.write
.option("compression", "snappy")
.parquet("/user/vagrant/test1/problem5/solution")
```

```scala
data
.filter($"product_price" > 1000.0)
.write
.option("compression", "snappy")
.format("avro")
.save("/user/vagrant/test1/problem5/solution2")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test1/problem5/solution
hdfs dfs -ls /user/vagrant/test1/problem5/solution2
```

## Problem 6

### Data

- data: `/user/vagrant/db/products`
- format: text

```bash
hdfs dfs -cat /user/vagrant/db/products/* | head
```

### Schema

```scala
product_id              int                                         
product_category_id     int                                         
product_name            string                                      
product_description     string                                      
product_price           double                                      
product_image           string   
```

### Output

```scala
val data = spark.read
.option("inferSchema", "true")
.csv("/user/vagrant/db/products")
.toDF("product_id", "product_category_id", "product_name", "product_description", "product_price", "product_image")
```

```scala
val result = data
.filter($"product_price" < 1000.0)
.filter($"product_name".like("%Vector Series%"))
```

```scala
val result = data
.filter($"product_price" < 1000.0)
.filter($"product_name".contains("Vector Series"))
```

```scala
result.write
.option("compression", "deflate")
.format("avro")
.save("/user/vagrant/test1/problem6/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test1/problem6/solution
```

## Problem 7

### Data

- table: "orders"
- database: "default"

```bash
hdfs dfs -cat /user/vagrant/db/orders/* | head
# 1,2013-07-25 00:00:00.0,11599,CLOSED
```

```scala
val data = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders")
.toDF("order_id", "order_date", "order_customer_id", "order_status")
data.write.mode("overwrite").saveAsTable("orders")
spark.sql("show tables").show()
spark.sql("describe orders").show()
spark.sql("select * from orders").show()
```

Rows: 68883

```scala
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|   orders|      false|
+--------+---------+-----------+

+-----------------+---------+-------+
|         col_name|data_type|comment|
+-----------------+---------+-------+
|         order_id|      int|   null|
|       order_date|timestamp|   null|
|order_customer_id|      int|   null|
|     order_status|   string|   null|
+-----------------+---------+-------+

+--------+-------------------+-----------------+---------------+
|order_id|         order_date|order_customer_id|   order_status|
+--------+-------------------+-----------------+---------------+
|       1|2013-07-25 00:00:00|            11599|         CLOSED|
|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|
+--------+-------------------+-----------------+---------------+
```

`data.count`: 3096

### Output

```scala
val data = spark.sql("""
select *
from default.orders
where order_date between '2013-01-01' and '2013-12-31'
""")
```

`data.count`: 3096

```scala
data.write.option("compression", "gzip").parquet("/user/vagrant/test1/problem7/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test1/problem7/solution
```

## Problem 8

### Data

- data: `/user/vagrant/db/categories`
- format: text
- delimited: comma

### Output

```scala
val data = spark.read
.option("inferSchema", "true")
.csv("/user/vagrant/db/categories")
.toDF("category_id", "category_department_id", "category_name")
```

```scala
// spark.sql("create table if not exists default.categories_replica(value string)")
```

```scala
data.map(row => row.mkString("|"))
.write
.format("text")
.option("compression", "uncompressed")
.mode("overwrite")
.saveAsTable("categories_replica")
```

### Hive

```scala
spark.sql("show tables").show()
spark.sql("describe categories_replica").show()
spark.sql("select * from categories_replica").show()
```

```scala
+--------+------------------+-----------+
|database|         tableName|isTemporary|
+--------+------------------+-----------+
| default|categories_replica|      false|
+--------+------------------+-----------+

+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|   value|   string|   null|
+--------+---------+-------+

+-------------+
|        value|
+-------------+
| 1|2|Football|
|   2|2|Soccer|
+-------------+
```