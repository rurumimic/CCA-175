# Test 2

```bash
spark-shell --master yarn --deploy-mode client --driver-memory 2g
```

## P1

### Data

- data: `/user/vagrant/db/categories`
- format: text
- delimited: comma

### Output

```scala
val data = spark.read.option("inferScheam", "true")
.csv("/user/vagrant/db/categories")
.toDF("id", "department_id", "name")
```

```scala
val result = data.select($"id".alias("category_id"), $"name".alias("category_name"))
```

```scala
// spark.sql("""
// create table if not exists default.categories_parquet (
//   category_id INT, category_name STRING
// ) stored as parquet
// """)
```

```scala
result.write
.format("parquet")
.mode("overwrite").saveAsTable("categories_parquet")
```

```scala
spark.sql("select * from categories_parquet").show()

+-----------+-------------------+
|category_id|      category_name|
+-----------+-------------------+
|          1|           Football|
|          2|             Soccer|
|          3|Baseball & Softball|
+-----------+-------------------+
```

## P2

### Data

- data: `/user/vagrant/db/categories`
- format: text
- delimited: comma

### Output

```scala
val data = spark.read.option("inferScheam", "true")
.csv("/user/vagrant/db/categories")
.toDF("category_id", "category_department_id", "category_name")
```

```scala
// spark.sqlContext.setConf("hive.exec.dynamic.partiion", "true")
// spark.sqlContext.setConf("hive.exec.dynamic.partiion.mode", "nonstrict")
```

```scala
data.write.mode("overwrite")
// .format("hive")
.partitionBy("category_department_id")
.saveAsTable("categories_partitioned2")
```

#### warehouse

```bash
ls ~/spark-warehouse/categories_partitioned

category_department_id=2
category_department_id=3
category_department_id=4
category_department_id=5
category_department_id=6
category_department_id=7
category_department_id=8
```

```scala
spark.sql("select * from categories_partitioned").show()

+-----------+-------------------+----------------------+
|category_id|      category_name|category_department_id|
+-----------+-------------------+----------------------+
|         30|   Men's Golf Clubs|                     6|
|         31| Women's Golf Clubs|                     6|
+-----------+-------------------+----------------------+
```

## P3

### Data

- data: `/user/vagrant/db/products_avro`
- format: avro
- compression: snappy

### Output

```scala
val data = spark.read.format("avro").load("/user/vagrant/db/products_avro")
```

```scala
// data.rdd.take(5).foreach(println)
// [1009,45,Diamond Fear No Evil Compound Bow Package,,599.99,http://images.acmesports.sports/Diamond+Fear+No+Evil+Compound+Bow+Package]
```

```scala
data.rdd.map(x => (x(0).toString, x.mkString(",")))
.saveAsSequenceFile("/user/vagrant/test2/problem3/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test2/problem3/solution
```

## P4

### Data

- data: `/user/vagrant/db/products_sequencefile`

### Output

```scala
import org.apache.hadoop.io.Text
val data = sc.sequenceFile("/user/vagrant/db/products_sequencefile", classOf[Text], classOf[Text])
```

```scala
val result = data.map(x => {
    val value = x._2.toString.split(","); 
    (value(0), value(1), value(2), value(3), value(4), value(5))
  }).toDF()

val result = data.map { case (x, y) =>  (x.toString(), y)}.collect()
```

```scala
result.write.option("compression", "snappy")
.parquet("/user/vagrant/test2/problem4/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test2/problem4/solution
```

## P5

### Data

- data: `/user/vagrant/db/customers-avro`
- format: avro

### Schema

```go
customer_id             int   
customer_fname          string
customer_lname          string
customer_email          string
```

### Output

```scala
val data = spark.read.format("avro").load("/user/vagrant/db/customers-avro")
```

```scala
val result = data.select($"customer_id", concat_ws(" ", substring($"customer_fname", 1, 1), $"customer_lname").alias("customer_name"))
```

```scala
data.createOrReplaceTempView("customers_view")
spark.sql("show tables").show()
spark.sql("describe customers_view").show()
```

```scala
val result = spark.sql("""
select customer_id, concat_ws(' ', substring(customer_fname, 1, 1), customer_lname) customer_name
from customers_view
""")
```

```scala
result.map(x => x.mkString("\t"))
.write
.option("compression", "bzip2")
.text("/user/vagrant/test2/problem5/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test2/problem5/solution
```

## P6

### Data

- data: `/user/vagrant/db/orders_parquet`
- format: parquet

### Output

```scala
val data = spark.read.parquet("/user/vagrant/db/orders_parquet")
```

```scala
val result = data
.filter($"order_status" === "PENDING")
.withColumn("order_date", to_date(from_unixtime($"order_date" / 1000)))
.filter($"order_date".contains("2013-07"))
.select($"order_date", $"order_status")
```

```scala
data.createOrReplaceTempView("orders_view")
val result = spark.sql("""
select to_date(from_unixtime(order_date/1000)) order_date, order_status
from orders_view
where order_status = "PENDING"
and to_date(from_unixtime(order_date/1000)) like "2013-07%"
""")
```

```scala
result.write.option("compression", "snappy").json("/user/vagrant/test2/problem6/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test2/problem6/solution
```

## P7

### Data

- table: "customers"
- database: "default"

```scala
val data = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
data.write.mode("overwrite").saveAsTable("customers")
spark.sql("show tables").show()
spark.sql("describe customers").show()
spark.sql("select * from customers").show()
```

### Output

```scala
val data = spark.sql("""
select *
from customers
where customer_fname like '%Rich%'
""")

data.write
.option("compression", "snappy")
.parquet("/user/vagrant/test2/problem7/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test2/problem7/solution
```

## P8

### Data

- data: `/user/vagrant/db/customers-tab-delimited`
- format: text
- delimited: tab

### Schema

```scala
customer_id             int   
customer_fname          string
customer_lname          string
customer_email          string
customer_password       string
customer_street         string
customer_city           string
customer_state          string
customer_zipcode        string
```

### Output

```scala
val data = spark.read
.option("delimiter", "\t")
.csv("/user/vagrant/db/customers-tab-delimited")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
data.createOrReplaceTempView("customer_view")
```

```scala
val result = spark.sql("""
select customer_state, count(1) cnt
from customer_view
where customer_fname like 'M%'
group by customer_state
""")
```

```scala
result.write
.option("compression", "gzip")
.parquet("/user/vagrant/test2/problem8/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test2/problem8/solution
```