# Test 3

```bash
spark-shell --master yarn --deploy-mode client --driver-memory 2g
```

## P1

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Schema

```go
order_id                int   
order_date              string
order_customer_id       int   
order_status            string
```

```go
customer_id             int   
customer_fname          string
customer_lname          string
customer_email          string
customer_password       string
customer_street         string
customer_city           string
customer_state          string
customer_zipcode        string
```

### Output

```bash
hdfs dfs -ls /user/vagrant/db/orders
hdfs dfs -ls /user/vagrant/db/customers
```

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val result = orders
.groupBy($"order_customer_id").count()
.filter($"count" > 5)
.join(customers, orders.col("order_customer_id") === customers("customer_id"), "inner")
.filter($"customer_fname".like("M%"))
.select("customer_fname", "customer_lname", "count")
.sort(desc("count"))
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")

val result = spark.sql("""
select c.customer_fname, c.customer_lname, count(1) cnt
from
orders_view o join customers_view c on o.order_customer_id = c.customer_id
where c.customer_fname like 'M%'
group by o.order_customer_id, c.customer_fname, c.customer_lname
having count(1) > 5
order by cnt desc
""").select("customer_fname", "customer_lname", "cnt")
```

```scala
result
.repartition(1)
// .coalesce(1)
.map(x => x.mkString("|"))
.write
.option("compression", "gzip")
.text("/user/vagrant/test3/problem1/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem1/solution
```

## P2

### Data

- data: `/user/vagrant/db/products`
- data: `/user/vagrant/db/categories`

### Output

```scala
val products = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/products").toDF("product_id", "product_category_id", "product_name", "product_description", "product_price", "product_image")

val categories = spark.read.option("inferSchema", true).csv("/user/vagrant/db/categories").toDF("category_id", "category_department_id", "category_name")
```

```scala
val result = products
.join(categories, products.col("product_category_id") === categories.col("category_id"), "inner")
.groupBy("category_name")
.agg(max("product_price").alias("max_price"), min("product_price").alias("min_price"), avg("product_price").alias("avg_price"))
```

```scala
products.createOrReplaceTempView("products_view")
categories.createOrReplaceTempView("categories_view")
val result = spark.sql("""
select c.category_name, round(max(p.product_price), 2) max_price, round(min(p.product_price), 2) min_price, round(avg(p.product_price), 2) avg_price
from products_view p join categories_view c on p.product_category_id = c.category_id
group by p.product_category_id, c.category_name
""").select("category_name", "max_price", "min_price", "avg_price")
```

```scala
result
.repartition(1)
// .coalesce(1)
.write
.option("compression", "deflate")
.json("/user/vagrant/test3/problem2/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem2/solution
```

## P3

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val fraud = orders.filter($"order_status" === "SUSPECTED_FRAUD")
.withColumn("order_month", substring($"order_date", 1, 7))
.groupBy("order_month")
.agg(count("order_month").alias("count"))
.withColumnRenamed("order_month", "order_date")
.sort(desc("order_date"))
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")
val result = spark.sql("""
select substring(order_date, 1, 7) order_date, order_status, count(1) count
from orders_view
where order_status = 'SUSPECTED_FRAUD'
group by substring(order_date, 1, 7), order_status
order by order_date desc
""").select("order_date", "count")
```

```scala
result
.repartition(1)
// .coalesce(1)
.write
.option("compression", "snappy")
.parquet("/user/vagrant/test3/problem3/solution")
```

## P4

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val result = orders
.withColumn("order_year", substring($"order_date", 1, 4))
.filter($"order_year" === "2014")
.filter($"order_status" === "COMPLETE")
.join(customers, orders.col("order_customer_id") === customers.col("customer_id"))
.groupBy("customer_id", "customer_fname", "customer_lname")
.agg(count("order_id").alias("order_count"))
.sort(desc("order_count"))
.select("customer_fname", "customer_lname", "order_count")
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")
val result = spark.sql("""
select c.customer_id, c.customer_fname, c.customer_lname, count(1) order_count
from orders_view o join customers_view c on o.order_customer_id = c.customer_id
where o.order_status = 'COMPLETE'
and year(o.order_date) = '2014'
group by c.customer_id, c.customer_fname, c.customer_lname
order by order_count desc
""").select("customer_fname", "customer_lname", "order_count")
```

```scala
result.repartition(1) // .coalesce(1)
.write.option("compression", "uncompressed")
.orc("/user/vagrant/test3/problem4/solution")
```

## P5

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val result = orders
.filter($"order_date".like("2013%"))
.filter($"order_status" === "COMPLETE")
.join(customers, orders.col("order_customer_id") === customers.col("customer_id"))
.groupBy("customer_id", "customer_fname", "customer_lname")
.agg(count("order_id").alias("order_count"))
.select("customer_fname", "customer_lname", "order_count")
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")
val result = spark.sql("""
select c.customer_id, c.customer_fname, c.customer_lname, count(1) order_count
from orders_view o join customers_view c on o.order_customer_id = c.customer_id
where o.order_status = 'COMPLETE'
and year(o.order_date) = '2013'
group by c.customer_id, c.customer_fname, c.customer_lname
""").select("customer_fname", "customer_lname", "order_count")
```

```scala
spark.sqlContext.setConf("hive.exec.dynamic.partition", "true")
spark.sqlContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")
```

```scala
result.write // .mode("overwrite")
// .format("hive")
.partitionBy("customer_fname")
.saveAsTable("customer_orders")
```

#### Table

```scala
spark.sql("select * from customer_orders").show()
```

## P6

- 1:N 관계
- [SELECT - OVER 절](https://docs.microsoft.com/ko-kr/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-ver15)

```sql
select
-- group by
o.order_date, o.order_id, 
-- 주문의 총 가격
sum(oi.order_item_subtotal) order_revenue,
-- 다시 그 날 주문한 것들을 기준. 날마다 각 주문의 총 가격의 평균
avg(sum(oi.order_item_subtotal)) over ( partition by o.order_date ) avg_revenue
-- Join
from orders_view o join order_items_view oi on o.order_id = oi.order_item_order_id
-- 1:N 관계 Join 이기 때문에 date와 id로 묶어야 한다.
-- 그리고 select에 다음 컬럼이 추가된다: o.order_date, o.order_id
group by o.order_date, o.order_id 
```

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/order_items`

### Schema

```go
order_item_id                   int           
order_item_order_id             int           
order_item_product_id           int           
order_item_quantity             tinyint       
order_item_subtotal             double        
order_item_product_price        double
```

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")
val order_items = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/order_items").toDF("order_item_id", "order_item_order_id", "order_item_product_id", "order_item_quantity", "order_item_subtotal", "order_item_product_price")
```

```scala
orders.createOrReplaceTempView("orders_view")
order_items.createOrReplaceTempView("order_items_view")

// year(t.order_date) order_year, 
// month(t.order_date) order_month, 
// day(t.order_date) order_day, 
// hour(t.order_date) order_hour, 

val result = spark.sql("""
select 
  to_date(t.order_date) order_date, 
  t.order_id,
  round(t.order_revenue, 2) order_revenue,
  round(t.avg_revenue, 2) avg_revenue
from 
  (select o.order_date, o.order_id, sum(oi.order_item_subtotal) order_revenue,
    avg(sum(oi.order_item_subtotal)) over ( partition by o.order_date ) avg_revenue
  from orders_view o join order_items_view oi on o.order_id = oi.order_item_order_id
  group by o.order_date, o.order_id) t
where order_revenue > avg_revenue
""")
```

```scala
result.write.option("compressoin", "bzip2").csv("/user/vagrant/test3/problem6/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem6/solution
```

## P7

- where: 우선 조건
- having: group by 후 새 테이블의 조건

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/order_items`
- data: `/user/vagrant/db/customers`

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")
val order_items = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/order_items").toDF("order_item_id", "order_item_order_id", "order_item_product_id", "order_item_quantity", "order_item_subtotal", "order_item_product_price")
val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
orders.createOrReplaceTempView("orders_view")
order_items.createOrReplaceTempView("order_items_view")
customers.createOrReplaceTempView("customers_view")
```

```scala
val result = spark.sql("""
select o.order_id, c.customer_fname, c.customer_lname, o.order_revenue
from
(select o.order_id, o.order_customer_id, sum(oi.order_item_subtotal) order_revenue
from orders_view o
join order_items_view oi on o.order_id = oi.order_item_order_id
group by o.order_id, o.order_customer_id, o.order_status
having o.order_status = 'COMPLETE') o
join customers_view c on o.order_customer_id = c.customer_id
where o.order_revenue > 500
order by o.order_id
""").select("customer_fname", "customer_lname", "order_revenue")
```

```scala
val result = spark.sql("""
select o.order_id, c.customer_fname, c.customer_lname, sum(oi.order_item_subtotal) order_revenue
from orders_view o
join order_items_view oi on o.order_id = oi.order_item_order_id
join customers_view c on o.order_customer_id = c.customer_id
where o.order_status = 'COMPLETE'
group by o.order_id, c.customer_fname, c.customer_lname
having order_revenue > 500
order by o.order_id
""").select("customer_fname", "customer_lname", "order_revenue")
```

```scala
val result = spark.sql("""
select o.order_id, c.customer_fname, c.customer_lname, o.order_revenue
from
(select o.order_id, o.order_customer_id, sum(oi.order_item_subtotal) order_revenue
from orders_view o
join order_items_view oi on o.order_id = oi.order_item_order_id
where o.order_status = 'COMPLETE'
group by o.order_id, o.order_customer_id, o.order_status) o
join customers_view c on o.order_customer_id = c.customer_id
where o.order_revenue > 500
order by o.order_id
""").select("customer_fname", "customer_lname", "order_revenue")
```

```scala
result.write.option("compression", "snappy").parquet("/user/vagrant/test3/problem7/solution")
```


#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem7/solution
```

## P8

### Data

- data: `/user/vagrant/db/products`
- data: `/user/vagrant/db/categories`
- data: `/user/vagrant/db/order_items`

### Output

```scala
val products = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/products").toDF("product_id", "product_category_id", "product_name", "product_description", "product_price", "product_image")
val categories = spark.read.option("inferSchema", true).csv("/user/vagrant/db/categories").toDF("category_id", "category_department_id", "category_name")
val order_items = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/order_items").toDF("order_item_id", "order_item_order_id", "order_item_product_id", "order_item_quantity", "order_item_subtotal", "order_item_product_price")
```

```scala
products.createOrReplaceTempView("products_view")
categories.createOrReplaceTempView("categories_view")
order_items.createOrReplaceTempView("order_items_view")
```

```scala
val result = spark.sql("""
select c.category_name, p.product_name, sum(oi.order_item_subtotal) product_revenue
from products_view p
join categories_view c on p.product_category_id = c.category_id
join order_items_view oi on p.product_id = oi.order_item_product_id
where c.category_name = 'Accessories'
group by p.product_id, p.product_category_id, p.product_name, c.category_name
order by product_revenue desc
limit 5
""").select("category_name", "product_name", "product_revenue")
```

```scala
val result = spark.sql("""
SELECT category_name, product_name, product_revenue
FROM     ( 
SELECT   c.category_name, p.product_name, Round(Sum(oi.order_item_subtotal), 2) product_revenue,
         Rank() OVER( partition BY c.category_name ORDER BY Round(Sum(oi.order_item_subtotal)) DESC) rnk
FROM     order_items_view oi 
JOIN     products_view p ON oi.order_item_product_id = p.product_id 
JOIN     categories_view c ON p.product_category_id = c.category_id
WHERE    c.category_name = 'Accessories'
GROUP BY c.category_name, p.product_name) t1 
WHERE    rnk <= 5
""")
```

```scala
result.repartition(1).map(row => row.mkString("|")).write.text("/user/vagrant/test3/problem8/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem8/solution
hdfs dfs -cat /user/vagrant/test3/problem8/solution/* | head
```
