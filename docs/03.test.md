# Test 3

```bash
spark-shell --master yarn --deploy-mode client --driver-memory 2g
```

## P1

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Schema

```go
order_id                int   
order_date              string
order_customer_id       int   
order_status            string
```

```go
customer_id             int   
customer_fname          string
customer_lname          string
customer_email          string
customer_password       string
customer_street         string
customer_city           string
customer_state          string
customer_zipcode        string
```

### Output

```bash
hdfs dfs -ls /user/vagrant/db/orders
hdfs dfs -ls /user/vagrant/db/customers
```

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val result = orders
.groupBy($"order_customer_id").count()
.filter($"count" > 5)
.join(customers, orders.col("order_customer_id") === customers("customer_id"), "inner")
.filter($"customer_fname".like("M%"))
.select("customer_fname", "customer_lname", "count")
.sort(desc("count"))
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")

val result = spark.sql("""
select c.customer_fname, c.customer_lname, count(1) cnt
from
orders_view o join customers_view c on o.order_customer_id = c.customer_id
where c.customer_fname like 'M%'
group by o.order_customer_id, c.customer_fname, c.customer_lname
having count(1) > 5
order by cnt desc
""").select("customer_fname", "customer_lname", "cnt")
```

```scala
result
.repartition(1)
// .coalesce(1)
.map(x => x.mkString("|"))
.write
.option("compression", "gzip")
.text("/user/vagrant/test3/problem1/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem1/solution
```

## P2

### Data

- data: `/user/vagrant/db/products`
- data: `/user/vagrant/db/categories`

### Output

```scala
val products = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/products").toDF("product_id", "product_category_id", "product_name", "product_description", "product_price", "product_image")

val categories = spark.read.option("inferSchema", true).csv("/user/vagrant/db/categories").toDF("category_id", "category_department_id", "category_name")
```

```scala
val result = products
.join(categories, products.col("product_category_id") === categories.col("category_id"), "inner")
.groupBy("category_name")
.agg(max("product_price").alias("max_price"), min("product_price").alias("min_price"), avg("product_price").alias("avg_price"))
```

```scala
products.createOrReplaceTempView("products_view")
categories.createOrReplaceTempView("categories_view")
val result = spark.sql("""
select c.category_name, round(max(p.product_price), 2) max_price, round(min(p.product_price), 2) min_price, round(avg(p.product_price), 2) avg_price
from products_view p join categories_view c on p.product_category_id = c.category_id
group by p.product_category_id, c.category_name
""").select("category_name", "max_price", "min_price", "avg_price")
```

```scala
result
.repartition(1)
// .coalesce(1)
.write
.option("compression", "deflate")
.json("/user/vagrant/test3/problem2/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem2/solution
```

## P3

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val fraud = orders.filter($"order_status" === "SUSPECTED_FRAUD")
.withColumn("order_month", substring($"order_date", 1, 7))
.groupBy("order_month")
.agg(count("order_month").alias("count"))
.withColumnRenamed("order_month", "order_date")
.sort(desc("order_date"))
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")
val result = spark.sql("""
select substring(order_date, 1, 7) order_date, order_status, count(1) count
from orders_view
where order_status = 'SUSPECTED_FRAUD'
group by substring(order_date, 1, 7), order_status
order by order_date desc
""").select("order_date", "count")
```

```scala
result
.repartition(1)
// .coalesce(1)
.write
.option("compression", "snappy")
.parquet("/user/vagrant/test3/problem3/solution")
```

## P4

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val result = orders
.withColumn("order_year", substring($"order_date", 1, 4))
.filter($"order_year" === "2014")
.filter($"order_status" === "COMPLETE")
.join(customers, orders.col("order_customer_id") === customers.col("customer_id"))
.groupBy("customer_id", "customer_fname", "customer_lname")
.agg(count("order_id").alias("order_count"))
.sort(desc("order_count"))
.select("customer_fname", "customer_lname", "order_count")
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")
val result = spark.sql("""
select c.customer_id, c.customer_fname, c.customer_lname, count(1) order_count
from orders_view o join customers_view c on o.order_customer_id = c.customer_id
where o.order_status = 'COMPLETE'
and year(o.order_date) = '2014'
group by c.customer_id, c.customer_fname, c.customer_lname
order by order_count desc
""").select("customer_fname", "customer_lname", "order_count")
```

```scala
result.repartition(1) // .coalesce(1)
.write.option("compression", "uncompressed")
.orc("/user/vagrant/test3/problem4/solution")
```

## P5

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/customers`

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")

val customers = spark.read.option("inferSchema", true).csv("/user/vagrant/db/customers").toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")
```

```scala
val result = orders
.filter($"order_date".like("2013%"))
.filter($"order_status" === "COMPLETE")
.join(customers, orders.col("order_customer_id") === customers.col("customer_id"))
.groupBy("customer_id", "customer_fname", "customer_lname")
.agg(count("order_id").alias("order_count"))
.select("customer_fname", "customer_lname", "order_count")
```

```scala
orders.createOrReplaceTempView("orders_view")
customers.createOrReplaceTempView("customers_view")
val result = spark.sql("""
select c.customer_id, c.customer_fname, c.customer_lname, count(1) order_count
from orders_view o join customers_view c on o.order_customer_id = c.customer_id
where o.order_status = 'COMPLETE'
and year(o.order_date) = '2013'
group by c.customer_id, c.customer_fname, c.customer_lname
""").select("customer_fname", "customer_lname", "order_count")
```

```scala
spark.sqlContext.setConf("hive.exec.dynamic.partition", "true")
spark.sqlContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")
```

```scala
result.write // .mode("overwrite")
// .format("hive")
.partitionBy("customer_fname")
.saveAsTable("customer_orders")
```

#### Table

```scala
spark.sql("select * from customer_orders").show()
```

## P6

### Data

- data: `/user/vagrant/db/orders`
- data: `/user/vagrant/db/order_items`

### Schema

```go
order_item_id                   int           
order_item_order_id             int           
order_item_product_id           int           
order_item_quantity             tinyint       
order_item_subtotal             double        
order_item_product_price        double
```

### Output

```scala
val orders = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/orders").toDF("order_id", "order_date", "order_customer_id", "order_status")
val order_items = spark.read.option("inferSchema", "true").csv("/user/vagrant/db/order_items").toDF("order_item_id", "order_item_order_id", "order_item_product_id", "order_item_quantity", "order_item_subtotal", "order_item_product_price")
```

```scala
orders.createOrReplaceTempView("orders_view")
order_items.createOrReplaceTempView("order_items_view")
val result = spark.sql("""
select 
  // year(t1.order_date) order_year, 
  // month(t1.order_date) order_month, 
  // day(t1.order_date) order_day, 
  // hour(t1.order_date) order_hour, 
  to_date(t1.order_date) order_date, 
  t1.order_id,
  round(t1.order_revenue, 2) order_revenue,
  round(t1.avg_revenue, 2) avg_revenue
from 
  (select o.order_date, o.order_id, sum(oi.order_item_subtotal) order_revenue,
    avg(sum(oi.order_item_subtotal)) over ( partition by o.order_date ) avg_revenue
  from orders_view o join order_items_view oi on o.order_id = oi.order_item_order_id
  group by o.order_date, o.order_id) t1
where order_revenue > avg_revenue
""")
```

```scala
result.write.option("compressoin", "bzip2").csv("/user/vagrant/test3/problem6/solution")
```

#### HDFS

```bash
hdfs dfs -ls /user/vagrant/test3/problem6/solution
```

## P7

### Data

### Schema

### Output

#### HDFS

## P8

### Data

### Schema

### Output

#### HDFS

